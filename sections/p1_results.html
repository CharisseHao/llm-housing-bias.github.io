<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">
    <h3>Prompt 1 Results </h3>
      <p class="result_title"><b>Differences by Gender</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_gender_for_each_model.png" class="center" alt="dist_img">
        <p>
          Our analysis of gender-based score differences found that while median scores remained consistent across groups, typically ranging from 80 to 85, variations emerged in score distribution and outliers across models. OpenAI’s models demonstrated stable mean scores between 77 and 83, whereas Google’s gemma-2-2b-it had a notably lower median of 65. InceptionAI’s Jais-Family-1P3B-Chat and Meta’s Meta-Llama-3-8B-Instruct showed wider score distributions, though their mean values remained comparable across gender groups. Most models exhibited a left-skewed distribution, indicating a higher frequency of higher scores.
        </p>
        <p>
        To assess gender differences within each model, we applied the Kruskal-Wallis test, as the dataset did not meet the assumptions for parametric tests. For models where significant differences were detected, Dunn’s test with Bonferroni correction was used for pairwise comparisons. OpenAI’s gpt-3.5-Turbo-0125 showed a significant difference only between the Gender-Neutral and Man groups, whereas Google’s gemma-2-2b-it, OpenAI’s gpt-4o series, and Meta’s models displayed more disparities, with two out of three gender comparisons reaching significance.
        </p>
        <p>
        Despite these statistical differences, their practical impact was minimal. The largest observed difference in median scores was zero, while the greatest mean difference was only 1.409 points on a 0 to 100 scale. This suggests that while some gender-based variations were statistically significant, they are unlikely to meaningfully affect real-world applications.</p>
        </p>

      <p class="result_title"><b>Differences by Race</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_race_for_each_model.png" class="center" alt="dist_img">
        <p> 
          Following our gender analysis, we examined racial differences in scores and found that while median values remained stable between 80 and 85 across most models, score distributions varied. OpenAI’s models maintained mean scores between 76 and 83 with low standard deviations, indicating minimal variation between racial groups. Among them, gpt-4o-Mini-2024-07-18 had the narrowest score range, suggesting greater consistency. In contrast, Google’s gemma-2-2b-it produced lower median scores of around 65 across all races, while InceptionAI’s Jais-Family-1P3B-Chat and Meta’s Meta-Llama-3-8B-Instruct had broader distributions. Notably, score ranges were wider than those observed in the gender-based analysis, with extreme outliers most frequently appearing in InceptionAI’s model.
        </p>
        <p>
          Despite the relative consistency in mean and median scores, statistical tests confirmed significant differences in score distributions across racial groups for multiple models. Since assumptions for parametric tests were not met, we applied the Kruskal-Wallis test followed by Dunn’s test with Bonferroni correction for models where differences were detected. OpenAI’s gpt-3.5-Turbo-0125 was the only model that showed no statistically significant racial disparities. InceptionAI’s model exhibited one significant difference between Chinese and Jewish groups, while Google’s gemma-2-2b-it had the most disparities, with 12 out of 28 comparisons reaching significance. OpenAI’s gpt-4o-2024-08-06 and Meta’s Meta-Llama-3-8B-Instruct followed closely, each with 10.
        </p>
        <p>
          The largest observed mean difference was 3.044 points in Meta’s Meta-Llama-3-8B-Instruct between the Jewish and None-control groups, while other models showed gaps ranging from 1.176 to 2.698 points. Although these differences were statistically significant, their small magnitude on a 0 to 100 scale suggests a limited real-world impact. However, some models exhibited more substantial median differences, with OpenAI’s gpt-4o models showing up to a 5-point gap and InceptionAI’s model displaying a 10-point difference. These larger median disparities could indicate a more noticeable impact in practical applications.
        </p>
      
      <p class="result_title"><b>Differences by Occupation</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_occupation_for_each_model.png" class="center" alt="dist_img">
        <p>
          Occupational labels introduced greater variation in scores than gender and race, with median and mean scores fluctuating more across occupations, suggesting that profession has a stronger influence on model outputs. OpenAI’s models and Google’s gemma-2-2b-it showed smaller distributions with less variability, whereas InceptionAI’s Jais-Family-1P3B-Chat and Meta’s Meta-Llama-3-8B-Instruct displayed wider ranges, with InceptionAI producing the most outliers. Notably, InceptionAI and Meta’s models exhibited left-skewed distributions with greater score disparities, while OpenAI’s models and Google’s gemma-2-2b-it had more uniform, normal distributions.
        </p>
        <p>
          The models’ sensitivity to occupational labels was evident in the consistently higher scores assigned to certain professions. Doctors ranked the highest, with mean scores ranging from 77.3 to 88.2 and median scores exceeding 85. Software engineers and teachers also scored highly, while unemployed individuals received the lowest scores, with means as low as 42.2 in OpenAI’s gpt-4o-2024-08-06, and scores ranging from 0 to 100 in InceptionAI’s model. Food service and retail workers also scored lower than professionals such as government employees and accountants.
        </p>
        <p>
          Given the failure of assumption tests, we applied the Kruskal-Wallis test, followed by Dunn’s test with Bonferroni correction. Google’s gemma-2-2b-it showed the most disparities, with 51 of 54 comparisons reaching significance, followed by OpenAI’s models, where 47 to 50 comparisons were significant, and Meta’s model, with 49. InceptionAI’s model exhibited fewer disparities, with only 8 significant comparisons. The most pronounced differences were between doctors and unemployed individuals, with mean gaps ranging from 26.76 for Google’s gemma-2-2b-it to 43.78 for OpenAI’s gpt-4o-2024-08-06. Median differences were even larger, reaching up to 45 points in OpenAI’s models, indicating a substantial impact compared to other demographic variables. In contrast, InceptionAI’s model showed minimal disparities, with a maximum mean difference of 4.01 and a median difference of 8 between doctors and retail associates.
        </p>
        <p>
          These findings highlight the potential for occupational biases in model outputs. The stark contrast between doctors and unemployed individuals suggests a weighting of professional status that may reflect biases in the training data. Since occupation correlates with socioeconomic factors, these disparities could reinforce inequalities in automated decision-making. Although InceptionAI’s model showed the least variation, it had the highest refusal rate at 79.11%. These results emphasize the need for further research into how models process occupational information and whether such biases lead to unfair outcomes.
        </p>
      
      <p class="result_title"><b>Differences by Living Status</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_living_status_for_each_model.png" class="center" alt="dist_img">
        <p>
          Finally, while living status influenced model scores, its effect was less pronounced than occupation. Across all models, scores exhibited left-skewed distributions with long tails, indicating that most candidates received relatively high scores. InceptionAI’s Jais-Family-1P3B-Chat produced the most outliers, reinforcing its broader and more dispersed scoring pattern.
        </p>
        <p>
          Unlike occupation, no single living status consistently received the highest score across models. OpenAI’s models assigned similar scores across categories, with median values around 85, though individuals living with a spouse scored slightly higher. Google’s gemma-2-2b-it produced lower overall scores, with medians between 60 and 65 and greater variation across categories. Meta’s Meta-Llama-3-8B-Instruct showed a wider range of scores, with individuals living alone or with a spouse averaging higher.
        </p>
        <p>
          As assumption tests failed, the Kruskal-Wallis test and Dunn’s test with Bonferroni correction was applied. Google’s gemma-2-2b-it exhibited the most differentiation, with 13 of 15 comparisons reaching significance and the largest mean difference of 5.357 between the control condition and individuals living with a pet. Meta’s model followed with 10 significant comparisons, while OpenAI’s models showed fewer differences, with 5 to 10 significant comparisons across models. InceptionAI’s model displayed no significant differences across groups, indicating more uniform treatment of this variable.
        </p>
        <p>
          Overall, while some models differentiated between living status groups, these differences were minor compared to occupation, with score variations ranging from less than 1 to just over 5 on a 100 point scale.</p>
        </p>
  </div>