<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">
    <h2>Results</h2>

    <h3>Data Cleaning and Exploratory Data Analysis</h3>
      <p> 
        We first conducted data cleaning and exploratory data analysis to standardize and better understand the dataset. Using regex, we extracted the numerical scores from the LLM responses, considering only those in the format “Score: X/100,” where X ranged from 0 to 100, as valid. If multiple scores appeared in a response, we calculated the average to maintain a single representative value per prompt. Responses were labeled as “refused” if the model declined to generate a response, failed to provide a score, or returned a score in an incorrect format.
      </p>
      <p>
        For the first prompt, 281,120 prompts were submitted, evenly distributed across six models, with 47,520 prompts per model. Of these, 37,842 (13.27%) were classified as refused. OpenAI’s gpt-3.5-Turbo-0125, gpt-4o-2024-08-06, and gpt-4o-Mini-2024-07-18, along with Meta’s Meta-Llama-3-8B-Instruct, had exceptionally low refusal counts, with fewer than 10 refusals each. Google’s gemma-2-2b-it had a slightly higher refusal count at 238 (0.5%), while Inception AI’s Jais-Family-1P3B-Chat had the highest refusal rate, rejecting 37,594 (79.11%) prompts. This high refusal rate suggests the model either follows much stricter response policies or struggles with adhering to the specified prompt format, with further analysis indicating formatting issues as the more likely cause.
      </p>
      <p>
        For the second prompt, 151,200 prompts were submitted, evenly distributed across seven models, with 21,600 per model. Of these, only 1,193 (0.78%) were classified as refused. OpenAI’s gpt-3.5-turbo-0125 and gpt-4o-mini-2024-07-18 had no refusals, while Meta’s Meta-Llama-3-8B-Instruct followed with just two. Google’s gemma-2-2b-it had a slightly higher refusal count at 36 (0.17%). OpenAI’s gpt-4o-2024-08-06 and Meta’s Llama-3.2-3B-Instruct had similar refusal rates, with 257 (1.19%) and 264 (1.22%) refusals, respectively. Microsoft’s Phi-3-mini-4k-instruct had the highest refusal rate, rejecting 634 (2.93%) prompts.
      </p>
      <p>
        Across both prompts, scores ranged from 0 to 100, with a mean score of 75.398 for the first prompt and 66.512 for the second, indicating generally favorably scoring tendencies. However, these single values do not fully capture variations in model performance, and subsequent sections provide a deeper analysis of model performance and statistical results.
      </p>
      
    <h3>Prompt 1 Results </h3>
      <p class="result_title"><b>Differences by Gender test</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_gender_for_each_model.png" class="center" alt="dist_img">
        <p>
          Our analysis of gender-based score differences found that while median scores remained consistent across groups, typically ranging from 80 to 85, variations emerged in score distribution and outliers across models. OpenAI’s models demonstrated stable mean scores between 77 and 83, whereas Google’s gemma-2-2b-it had a notably lower median of 65. InceptionAI’s Jais-Family-1P3B-Chat and Meta’s Meta-Llama-3-8B-Instruct showed wider score distributions, though their mean values remained comparable across gender groups. Most models exhibited a left-skewed distribution, indicating a higher frequency of higher scores.
        </p>
        <p>
        To assess gender differences within each model, we applied the Kruskal-Wallis test, as the dataset did not meet the assumptions for parametric tests. For models where significant differences were detected, Dunn’s test with Bonferroni correction was used for pairwise comparisons. OpenAI’s gpt-3.5-Turbo-0125 showed a significant difference only between the Gender-Neutral and Man groups, whereas Google’s gemma-2-2b-it, OpenAI’s gpt-4o series, and Meta’s models displayed more disparities, with two out of three gender comparisons reaching significance.
        </p>
        <p>
        Despite these statistical differences, their practical impact was minimal. The largest observed difference in median scores was zero, while the greatest mean difference was only 1.409 points on a 0 to 100 scale. This suggests that while some gender-based variations were statistically significant, they are unlikely to meaningfully affect real-world applications.</p>
        </p>

      <p class="result_title"><b>Differences by Race</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_race_for_each_model.png" class="center" alt="dist_img">
        <p> 
          Following our gender analysis, we examined racial differences in scores and found that while median values remained stable between 80 and 85 across most models, score distributions varied. OpenAI’s models maintained mean scores between 76 and 83 with low standard deviations, indicating minimal variation between racial groups. Among them, gpt-4o-Mini-2024-07-18 had the narrowest score range, suggesting greater consistency. In contrast, Google’s gemma-2-2b-it produced lower median scores of around 65 across all races, while InceptionAI’s Jais-Family-1P3B-Chat and Meta’s Meta-Llama-3-8B-Instruct had broader distributions. Notably, score ranges were wider than those observed in the gender-based analysis, with extreme outliers most frequently appearing in InceptionAI’s model.
        </p>
        <p>
          Despite the relative consistency in mean and median scores, statistical tests confirmed significant differences in score distributions across racial groups for multiple models. Since assumptions for parametric tests were not met, we applied the Kruskal-Wallis test followed by Dunn’s test with Bonferroni correction for models where differences were detected. OpenAI’s gpt-3.5-Turbo-0125 was the only model that showed no statistically significant racial disparities. InceptionAI’s model exhibited one significant difference between Chinese and Jewish groups, while Google’s gemma-2-2b-it had the most disparities, with 12 out of 28 comparisons reaching significance. OpenAI’s gpt-4o-2024-08-06 and Meta’s Meta-Llama-3-8B-Instruct followed closely, each with 10.
        </p>
        <p>
          The largest observed mean difference was 3.044 points in Meta’s Meta-Llama-3-8B-Instruct between the Jewish and None-control groups, while other models showed gaps ranging from 1.176 to 2.698 points. Although these differences were statistically significant, their small magnitude on a 0 to 100 scale suggests a limited real-world impact. However, some models exhibited more substantial median differences, with OpenAI’s gpt-4o models showing up to a 5-point gap and InceptionAI’s model displaying a 10-point difference. These larger median disparities could indicate a more noticeable impact in practical applications.
        </p>
      
      <p class="result_title"><b>Differences by Occupation</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_occupation_for_each_model.png" class="center" alt="dist_img">
        <p>
          Occupational labels introduced greater variation in scores than gender and race, with median and mean scores fluctuating more across occupations, suggesting that profession has a stronger influence on model outputs. OpenAI’s models and Google’s gemma-2-2b-it showed smaller distributions with less variability, whereas InceptionAI’s Jais-Family-1P3B-Chat and Meta’s Meta-Llama-3-8B-Instruct displayed wider ranges, with InceptionAI producing the most outliers. Notably, InceptionAI and Meta’s models exhibited left-skewed distributions with greater score disparities, while OpenAI’s models and Google’s gemma-2-2b-it had more uniform, normal distributions.
        </p>
        <p>
          The models’ sensitivity to occupational labels was evident in the consistently higher scores assigned to certain professions. Doctors ranked the highest, with mean scores ranging from 77.3 to 88.2 and median scores exceeding 85. Software engineers and teachers also scored highly, while unemployed individuals received the lowest scores, with means as low as 42.2 in OpenAI’s gpt-4o-2024-08-06, and scores ranging from 0 to 100 in InceptionAI’s model. Food service and retail workers also scored lower than professionals such as government employees and accountants.
        </p>
        <p>
          Given the failure of assumption tests, we applied the Kruskal-Wallis test, followed by Dunn’s test with Bonferroni correction. Google’s gemma-2-2b-it showed the most disparities, with 51 of 54 comparisons reaching significance, followed by OpenAI’s models, where 47 to 50 comparisons were significant, and Meta’s model, with 49. InceptionAI’s model exhibited fewer disparities, with only 8 significant comparisons. The most pronounced differences were between doctors and unemployed individuals, with mean gaps ranging from 26.76 for Google’s gemma-2-2b-it to 43.78 for OpenAI’s gpt-4o-2024-08-06. Median differences were even larger, reaching up to 45 points in OpenAI’s models, indicating a substantial impact compared to other demographic variables. In contrast, InceptionAI’s model showed minimal disparities, with a maximum mean difference of 4.01 and a median difference of 8 between doctors and retail associates.
        </p>
        <p>
          These findings highlight the potential for occupational biases in model outputs. The stark contrast between doctors and unemployed individuals suggests a weighting of professional status that may reflect biases in the training data. Since occupation correlates with socioeconomic factors, these disparities could reinforce inequalities in automated decision-making. Although InceptionAI’s model showed the least variation, it had the highest refusal rate at 79.11%. These results emphasize the need for further research into how models process occupational information and whether such biases lead to unfair outcomes.
        </p>
      
      <p class="result_title"><b>Differences by Living Status</b></p>
        <img src="static/images/p1plots/variables_all_models/distribution_of_responses_by_living_status_for_each_model.png" class="center" alt="dist_img">
        <p>
          No single living status category consistently received the highest scores across models. OpenAI’s models assigned similar scores across groups, while Google’s gemma-2-2b-it and Meta’s Meta-Llama-3-8B-Instruct showed more variation, with individuals living alone or with a spouse tending to score higher.
        </p>

        <p>
          Statistical tests confirmed significant differences in some models, with Google’s gemma-2-2b-it showing the most variation (13 out of 15 significant comparisons) and the largest mean difference of 5.36. Meta’s model followed closely, while OpenAI’s models exhibited smaller differences, with maximum mean gaps below 1.6. InceptionAI’s jais-family-1p3b-chat showed no significant differences, indicating uniform treatment of living status.
        </p>
        <p>
          Overall, while some models distinguished between living status groups, these differences were relatively small, especially compared to occupation-related disparities.
        </p>

      <h3>Prompt 2 Results </h3>
      <br>
      <h4>Differences by Race</h4> 
      <p>
        In the second housing prompt, racial score differences varied notably among LLMs. OpenAI’s gpt-3.5-Turbo-0125 and gpt-4o-Mini-2024-07-18 showed relatively tight ranges with medians clustered between 60 and 85, whereas Google’s gemma-2-2b-it produced a lower median and broader distribution. Meta’s Llama models and Microsoft’s Phi-3-mini-4k-instruct also displayed wider score spreads, indicating more inconsistency in how these models evaluated different racial groups compared to OpenAI’s offerings.
      </p>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_race.png" alt="dist_img">
      <p>
        Because the data did not meet normality assumptions, the Kruskal-Wallis test revealed mixed results. Google’s gemma-2-2b-it and OpenAI’s gpt-4o-Mini-2024-07-18 showed no statistically significant racial differences, while OpenAI’s gpt-3.5-Turbo-0125, OpenAI’s gpt-4o-2024-08-06, and Meta’s Llama-3.2-3B-Instruct did. Pairwise comparisons in these latter models identified notable gaps between None-control and specific racial groups. Such inconsistencies underscore the potential for bias in certain LLMs and the need for further research on how training data and methodologies influence these disparities.
      </p>

      <h4>Differences by Gender</h4>
      <p> 
        Most models scored women, men, and gender-neutral names similarly, with median scores often aligning across gender conditions. However, Google’s gemma-2-2b-it gave notably lower median scores (around 40) while still achieving some high outliers, and Meta-Llama-3-2-3B-Instruct produced a slightly lower median for women despite similar average scores for men and women. Interestingly, every model scored the gender-neutral name higher on average than both women and men. While OpenAI’s gpt-3.5-Turbo-0125 had a relatively narrow score distribution (roughly 40–97), Google’s gemma-2-2b-it ranged from 0 to 100 and was the only model showing pronounced right skew. Though these differences are modest, they could still influence real-world decisions in tenant evaluations.   
      </p>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_gender.png" alt="dist_img">
      <p> 
        Since the models did not meet normality and equal variance assumptions, the Kruskal-Wallis test showed that nearly all models—except Microsoft’s Phi-3-mini-4k-instruct—produced statistically significant differences in scores between gender conditions. While the largest absolute mean difference was only 1.314, Meta’s Llama-3.2-3B-Instruct showed a 5-point median gap favoring men over women. Even a 5-point variance on a 0–100 scale could have a meaningful impact in real-world housing decisions, underscoring the importance of monitoring potential biases in LLM-generated tenant scores.
      </p>
      <h4>Differences by Credit Score </h4>
      <p> Across different credit score tiers, LLMs vary in their tenant scoring consistency. OpenAI’s gpt-4o-2024-08-06 and gpt-4o-Mini-2024-07-18 generally produce tightly clustered scores with fewer outliers, suggesting a more uniform handling of credit data. Meanwhile, Google’s gemma-2-2b-it and Meta’s Llama-3.2-3B-Instruct exhibit broader distributions, implying greater variability in how they assess financial credibility. These discrepancies highlight the need to scrutinize how each model interprets credit profiles in real-world housing applications.
      </p>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_credit_scores.png" alt="dist_img">
      <p>
        Across all tested LLMs, individuals with higher credit scores (750–850) consistently received more favorable tenant evaluations. While this trend aligns with traditional financial assessments, it also raises questions about whether LLMs amplify existing barriers to housing for those with lower credit. In particular, Google’s gemma-2-2b-it and Meta’s Llama models showed broader score distributions for lower-credit groups, indicating greater variability and potential inconsistency in decision-making. The Kruskal-Wallis test revealed statistically significant differences between all credit score categories, underscored by strong pairwise comparisons—especially when comparing lower (500) and higher (750 or 850) scores. This pattern held true across multiple LLMs, reinforcing that these models, as currently configured, may systematically favor applicants with stronger credit profiles while treating borderline or unknown-credit applicants inconsistently.
      </p>
  
      <h4>Differences by Eviction Status</h4>
      <p>
        Across all models, tenants with any mention of eviction received notably lower scores, although the extent varied by model. Google’s gemma-2-2b-it, Meta’s Llama-3.2-3B-Instruct and Llama-3.8B-Instruct, and Microsoft’s Phi-3-mini-4k-instruct showed wide score ranges (roughly 20 to 95), whereas OpenAI’s models were more consistent. Even dismissed eviction cases tended to score lower than having no eviction history, suggesting that simply flagging “eviction”—regardless of fault—could negatively impact scores. Applicants evicted six years ago were treated slightly more favorably than those recently evicted, but overall, prior evictions consistently ranked as a high-risk factor across models.  
      </p>
      <img src="static/images/p2plots/distribution_of_responses_for_each_model_model_eviction.png" alt="dist_img">
      <p>
        After testing eviction status via the Kruskal-Wallis and Dunn’s tests, all pairwise comparisons were statistically significant except one—OpenAI’s gpt-3.5-Turbo-0125 did not distinguish between “no record of eviction” and “gone to eviction court but dismissed.” Notably, “gone to eviction court but dismissed” often scored lower than “no record,” with mean differences ranging from 0.66 up to nearly 15 points. While this bias may seem minor for some models, a consistent gap of 4–7 points—or even up to 18 when comparing medians—could meaningfully impact real-world applications. Therefore, any system using LLM-based tenant scoring should take steps to mitigate unfair penalties associated with even dismissed evictions.  
      </p>

  </div>