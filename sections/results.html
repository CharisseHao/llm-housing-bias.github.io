<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">
    <h2>Results</h2>

    <h3>Data Cleaning and Exploratory Data Analysis</h3>
      <p> 
        We first conducted data cleaning and exploratory data analysis to standardize and better understand the dataset. Using regex, we extracted the numerical scores from the LLM responses, considering only those in the format “Score: X/100,” where X ranged from 0 to 100, as valid. If multiple scores appeared in a response, we calculated the average to maintain a single representative value per prompt. Responses were labeled as “refused” if the model declined to generate a response, failed to provide a score, or returned a score in an incorrect format.
      </p>
      <p>
        For the first prompt, 281,120 prompts were submitted, evenly distributed across six models, with 47,520 prompts per model. Of these, 37,842 (13.27%) were classified as refused. OpenAI’s gpt-3.5-Turbo-0125, gpt-4o-2024-08-06, and gpt-4o-Mini-2024-07-18, along with Meta’s Meta-Llama-3-8B-Instruct, had exceptionally low refusal counts, with fewer than 10 refusals each. Google’s gemma-2-2b-it had a slightly higher refusal count at 238 (0.5%), while Inception AI’s Jais-Family-1P3B-Chat had the highest refusal rate, rejecting 37,594 (79.11%) prompts. This high refusal rate suggests the model either follows much stricter response policies or struggles with adhering to the specified prompt format, with further analysis indicating formatting issues as the more likely cause.
      </p>
      <p>
        For the second prompt, 151,200 prompts were submitted, evenly distributed across seven models, with 21,600 per model. Of these, only 1,193 (0.78%) were classified as refused. OpenAI’s gpt-3.5-turbo-0125 and gpt-4o-mini-2024-07-18 had no refusals, while Meta’s Meta-Llama-3-8B-Instruct followed with just two. Google’s gemma-2-2b-it had a slightly higher refusal count at 36 (0.17%). OpenAI’s gpt-4o-2024-08-06 and Meta’s Llama-3.2-3B-Instruct had similar refusal rates, with 257 (1.19%) and 264 (1.22%) refusals, respectively. Microsoft’s Phi-3-mini-4k-instruct had the highest refusal rate, rejecting 634 (2.93%) prompts.
      </p>
      <p>
        Across both prompts, scores ranged from 0 to 100, with a mean score of 75.398 for the first prompt and 66.512 for the second, indicating generally favorably scoring tendencies. However, these single values do not fully capture variations in model performance, and subsequent sections provide a deeper analysis of model performance and statistical results.
      </p>
  </div>