<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">
    <h2>Conclusion</h2>
    <p>
      This study explores biases in tenant scores generated by large language models (LLMs) using two prompts: one focused on demographic details like gender, race, occupation, and living status in an informal email format, and the other on credit scores and eviction history in a formal application format. Our findings reveal significant biases in LLM tenant scoring, with variability in how models handle different attributes, often disadvantaging certain groups.
    </p>
    <p>
      Notably, occupation produced the most significant disparities in the first prompt, with professions such as doctors receiving consistently higher scores and unemployed applicants scoring lower. In the second prompt, eviction history and credit score also caused biases, with prior evictions—regardless of outcome—resulting in lower scores, while higher credit scores generally correlated with better ratings. These biases, both subtle and pronounced, raise concerns about the potential for LLMs to reinforce existing inequalities in housing.
    </p>
    <p>
      Although some models exhibited more consistent results, others displayed considerable variability, raising concerns about the unpredictability of LLM decisions in real-world applications. InceptionAI’s Jais-Family-1P3B-Chat had the highest refusal rate at almost 80 percent due to its inability to follow the prompt, while Google’s gemma-2-2b-it consistently produced lower scores than other models.
    </p>
    <p>
      While the study offers valuable insights, its limitations—such as its focus on San Diego, CA, and exclusion of key financial details—suggest the need for further research. Future studies should:
    </p>
    <ul>
      <li>
        <strong>Broaden the scope</strong>: expand geographic and demographic diversity
      </li>
      <li>
        <strong>Enhance prompt complexity</strong>: test different prompt structures
      </li>
      <li>
        <strong>Benchmark against human decisions</strong>: compare LLM biases to human decision-making in housing contexts
      </li>
    </ul>
    <p>
      Ultimately, this research underscores the importance of monitoring LLMs’ impact on housing decisions and calls for future work to mitigate algorithmic biases, inform policy, and raise awareness about potential discrimination in LLM-driven tenant screening systems.
    <p>
</div>