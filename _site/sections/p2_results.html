<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">
      <h3>Prompt 2 Results </h3>
      <p class="result_title"><b>Differences by Gender</b></p>
        <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_gender_for_each_model.png" class="center" alt="dist_img">
        <p> 
          For prompt 2, our analysis of tenant score differences based on gender revealed slight biases, with most models assigning similar mean and median scores across gender conditions and displaying relatively normal distributions. OpenAI’s gpt-3.5-Turbo-0125 had the highest mean scores, around 81, with medians at 85, while Google’s gemma-2-2b-it had the lowest mean scores at 50 and medians at 40. Score distributions also varied, with OpenAI’s gpt-3.5-Turbo-0125 having the narrowest range from 40 to 97, while Google’s gemma-2-2b-it had the widest, spanning from 0 to 100.
        </p>
        <p>
          Since models failed normality and variance assumptions, we applied the Kruskal-Wallis test, followed by Dunn’s test where needed. Microsoft’s Phi-3-mini-4k-instruct was the only model with no significant differences detected by the Kruskal-Wallis test. Most other models found significant differences between the Gender-Neutral and Man groups, as well as the Man and Woman groups, except for OpenAI’s gpt-4o-Mini-2024-07-18, which only found bias between the Gender-Neutral and Woman groups. While the largest absolute mean difference was only 1.314, Meta’s Meta-Llama-3-2-3B-Instruct showed a median difference of 5, favoring Men over Women—enough to potentially impact housing opportunities in close-scoring applications.
        </p>

      <p class="result_title"><b>Differences by Race</b></p>
        <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_race_for_each_model.png" class="center" alt="dist_img">
        <p>
          Racial scores exhibited more variation among models than gender. Although all models had relatively similar means and medians across racial groups, their distributions were much wider than those of previously analyzed variables. Based on the boxenplots, Google’s gemma-2-2b-it and Microsoft’s Phi-3-mini-4k displayed the widest distributions, with Google’s model once again producing the lowest mean scores at around 50 and median scores at 40. This broad distribution of scores across all models suggests greater inconsistency in racial group scoring, potentially influenced by other tested variables.
        </p>
        <p>
          Due to the failure to meet parametric assumption tests, we applied non-parametric tests to assess statistical significance. While Google’s gemma-2-2b-it, OpenAI’s gpt-4o-Mini-2024-07-18, and Microsoft’s Phi-3-mini-4k-instruct did not detect significant differences, other models found notable disparities in scores across racial groups. Dunn’s test highlighted these differences: OpenAI’s gpt-3.5-Turbo-0125 failed 12 out of 28 tests, with Anglo, Arabic, Black, and Hispanic groups receiving significantly different scores compared to the None-Control condition. OpenAI’s gpt-4o-2024-08-06 and Meta’s Llama-3.2-3B-Instruct each failed 10 tests, while Meta’s Meta-Llama-3-8B-Instruct failed 6.
        </p>
        <p>
          These findings suggest that racial groups are treated inconsistently across models, with some exhibiting significant disparities while others show minimal differences. The variation in statistical significance underscores potential biases and highlights the need for further investigation into how training data and model methodologies influence racial scoring outcomes.
        </p>
        
      <p class="result_title"><b>Differences by Eviction Status</b></p>
      <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_eviction_for_each_model.png"class="center" alt="dist_img">
        <p>
          Analyzing eviction history also revealed significant differences across all models. Score distributions varied widely, with Google’s gemma-2-2b-it, Meta’s Llama-3.2-3B-Instruct and Llama-3.8B-Instruct, and Microsoft’s Phi-3-mini-4k-instruct ranging from below 20 to above 95, while OpenAI’s models exhibited less variance. Across all models, the variables ‘previously been evicted’ and ‘previously been evicted 6 years ago’ received the lowest scores, though the latter scored slightly higher on average, aligning with landlord risk assessments. Additionally, the ‘gone to eviction court but case was dismissed’ variable scored lower than ‘no record of eviction’ across all models, with OpenAI’s gpt-3.5-Turbo-0125 being the only model to score them similarly. This suggests that most models penalize applicants at the mention of eviction, even if they were not at fault.
        </p>
        <p>
          Since models failed normality and variance assumptions, we applied the Kruskal-Wallis test, followed by Dunn’s test for pairwise comparisons. All eviction history comparisons were statistically significant, except for OpenAI’s gpt-3.5-Turbo-0125, which found no significant difference between the None-Control and ‘gone to eviction court but case was dismissed’ groups. Absolute mean score differences ranged from 0.66 to 14.96, with most models showing a 4 to 7-point difference, while absolute median differences ranged from 0 to 18, with most models falling between 2 and 5 points. Despite these significant results, the primary concern remains the discrepancy between ‘gone to eviction court but case was dismissed’ and ‘no record of eviction’, as this confirms bias against individuals who have faced eviction court proceedings, even when not at fault.
        </p>

      <p class="result_title"><b>Differences by Credit Score</b></p>
      <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_credit_scores_for_each_model.png" class="center" alt="dist_img">
        <p>
          Lastly, we examined how models evaluate individuals based on financial history and found significant differences in how they handle credit groups. Some models, like OpenAI’s gpt-4o-2024-08-06 and gpt-4o-Mini-2024-07-18, produced more consistent results with tighter interquartile ranges, while Google’s gemma-2-2b-it and Meta’s Llama-3.2-3B-Instruct displayed broader distributions, suggesting more variability and inconsistencies in how LLMs interpret financial credibility.
        </p>
        <p>
          A clear trend emerged where higher-credit individuals, with scores from 750 to 850, received higher median scores across most models, mirroring traditional financial assessments. However, lower-credit groups, particularly those with scores between 500 and 650, were treated inconsistently, with some models showing steep declines in scores. For instance, OpenAI’s gpt-3.5-Turbo-0125 demonstrated a more uniform approach, while Google’s gemma-2-2b-it and Meta’s models showed greater variation. The inclusion of the None-control group also highlighted differing treatment across models, pointing to potential biases.
        </p>
        <p>
          The Kruskal-Wallis test confirmed significant differences between credit groups, with pairwise comparisons revealing notable disparities, particularly between higher and lower credit scores. .These findings were consistent across models, including Google’s gemma-2-2b-it, OpenAI’s GPT series, and Meta’s Llama models.
        </p>
  </div>