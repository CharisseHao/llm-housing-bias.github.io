<div class="section-content">
  <link rel="stylesheet" href="static/css/common.css">
      <h3>Prompt 2 Results </h3>
      <p class="result_title"><b>Differences by Gender</b></p>
        <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_gender_for_each_model.png" class="center" alt="dist_img">
        <p> 
          For prompt 2, our analysis of tenant score differences based on gender revealed slight biases, with most models assigning similar mean and median scores across gender conditions and displaying relatively normal distributions. OpenAI’s gpt-3.5-Turbo-0125 had the highest mean scores, around 81, with medians at 85, while Google’s gemma-2-2b-it had the lowest mean scores at 50 and medians at 40. Score distributions also varied, with OpenAI’s gpt-3.5-Turbo-0125 having the narrowest range from 40 to 97, while Google’s gemma-2-2b-it had the widest, spanning from 0 to 100.
        </p>
        <p>
          Since models failed normality and variance assumptions, we applied the Kruskal-Wallis test, followed by Dunn’s test where needed. Microsoft’s Phi-3-mini-4k-instruct was the only model with no significant differences detected by the Kruskal-Wallis test. Most other models found significant differences between the Gender-Neutral and Man groups, as well as the Man and Woman groups, except for OpenAI’s gpt-4o-Mini-2024-07-18, which only found bias between the Gender-Neutral and Woman groups. While the largest absolute mean difference was only 1.314, Meta’s Meta-Llama-3-2-3B-Instruct showed a median difference of 5, favoring Men over Women—enough to potentially impact housing opportunities in close-scoring applications.
        </p>

      <p class="result_title"><b>Differences by Race</b></p>
        <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_race_for_each_model.png" class="center" alt="dist_img">
        <p>
          Racial scores exhibited more variation among models than gender. Although all models had relatively similar means and medians across racial groups, their distributions were much wider than those of previously analyzed variables. Based on the boxenplots, Google’s gemma-2-2b-it and Microsoft’s Phi-3-mini-4k displayed the widest distributions, with Google’s model once again producing the lowest mean scores at around 50 and median scores at 40. This broad distribution of scores across all models suggests greater inconsistency in racial group scoring, potentially influenced by other tested variables.
        </p>
        <p>
          Due to the failure to meet parametric assumption tests, we applied non-parametric tests to assess statistical significance. While Google’s gemma-2-2b-it, OpenAI’s gpt-4o-Mini-2024-07-18, and Microsoft’s Phi-3-mini-4k-instruct did not detect significant differences, other models found notable disparities in scores across racial groups. Dunn’s test highlighted these differences: OpenAI’s gpt-3.5-Turbo-0125 failed 12 out of 28 tests, with Anglo, Arabic, Black, and Hispanic groups receiving significantly different scores compared to the None-Control condition. OpenAI’s gpt-4o-2024-08-06 and Meta’s Llama-3.2-3B-Instruct each failed 10 tests, while Meta’s Meta-Llama-3-8B-Instruct failed 6.
        </p>
        <p>
          These findings suggest that racial groups are treated inconsistently across models, with some exhibiting significant disparities while others show minimal differences. The variation in statistical significance underscores potential biases and highlights the need for further investigation into how training data and model methodologies influence racial scoring outcomes.
        </p>
        
      <p class="result_title"><b>Differences by Eviction Status</b></p>
      <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_eviction_for_each_model.png"class="center" alt="dist_img">
        <p>
          Analyzing eviction history also revealed significant differences across all models. Score distributions varied widely, with Google’s gemma-2-2b-it, Meta’s Llama-3.2-3B-Instruct and Llama-3.8B-Instruct, and Microsoft’s Phi-3-mini-4k-instruct ranging from below 20 to above 95, while OpenAI’s models exhibited less variance. Across all models, the variables ‘previously been evicted’ and ‘previously been evicted 6 years ago’ received the lowest scores, though the latter scored slightly higher on average, aligning with landlord risk assessments. Additionally, the ‘gone to eviction court but case was dismissed’ variable scored lower than ‘no record of eviction’ across all models, with OpenAI’s gpt-3.5-Turbo-0125 being the only model to score them similarly. This suggests that most models penalize applicants at the mention of eviction, even if they were not at fault.
        </p>
        <p>
          Since models failed normality and variance assumptions, we applied the Kruskal-Wallis test, followed by Dunn’s test for pairwise comparisons. All eviction history comparisons were statistically significant, except for OpenAI’s gpt-3.5-Turbo-0125, which found no significant difference between the None-Control and ‘gone to eviction court but case was dismissed’ groups. Absolute mean score differences ranged from 0.66 to 14.96, with most models showing a 4 to 7-point difference, while absolute median differences ranged from 0 to 18, with most models falling between 2 and 5 points. Despite these significant results, the primary concern remains the discrepancy between ‘gone to eviction court but case was dismissed’ and ‘no record of eviction’, as this confirms bias against individuals who have faced eviction court proceedings, even when not at fault.
        </p>

      <p class="result_title"><b>Differences by Credit Score - WIP</b></p>
      <img src="static/images/p2plots/variables_all_models/distribution_of_responses_by_credit_scores_for_each_model.png" class="center" alt="dist_img">
        <p> Across different credit score tiers, LLMs vary in their tenant scoring consistency. OpenAI’s gpt-4o-2024-08-06 and gpt-4o-Mini-2024-07-18 generally produce tightly clustered scores with fewer outliers, suggesting a more uniform handling of credit data. Meanwhile, Google’s gemma-2-2b-it and Meta’s Llama-3.2-3B-Instruct exhibit broader distributions, implying greater variability in how they assess financial credibility. These discrepancies highlight the need to scrutinize how each model interprets credit profiles in real-world housing applications.
        </p>
        <p>
          Across all tested LLMs, individuals with higher credit scores (750–850) consistently received more favorable tenant evaluations. While this trend aligns with traditional financial assessments, it also raises questions about whether LLMs amplify existing barriers to housing for those with lower credit. In particular, Google’s gemma-2-2b-it and Meta’s Llama models showed broader score distributions for lower-credit groups, indicating greater variability and potential inconsistency in decision-making. The Kruskal-Wallis test revealed statistically significant differences between all credit score categories, underscored by strong pairwise comparisons—especially when comparing lower (500) and higher (750 or 850) scores. This pattern held true across multiple LLMs, reinforcing that these models, as currently configured, may systematically favor applicants with stronger credit profiles while treating borderline or unknown-credit applicants inconsistently.
        </p>

  </div>