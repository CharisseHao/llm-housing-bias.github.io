<div class="section-content">
    <h2>Abstract</h2>
    <p>
      As the United States housing crisis intensifies, competition for housing has surged, increasing the likelihood that landlords and other decision-makers will turn to technological tools, including large language models (LLMs) like ChatGPT, to aid in their decision-making processes. However, these models often reflect and perpetuate societal biases present in their training data, potentially influencing critical housing-related decisions, such as eligibility assessments, tenant screening, and eviction risk evaluations. This study systematically examines how biases manifest in LLM-generated responses to housing-related prompts, focusing on disparities across race, gender, economic status, and other factors. Using an adapted algorithm audit framework, we generate prompts with varying demographic details to assess potential biases in LLM outputs. These prompts are submitted to selected models, with results analyzed using statistical methods. The findings are presented through a report, poster, and interactive website, allowing users to explore bias patterns firsthand. By investigating the impact of different candidate characteristics on LLM responses, this project contributes to the discourse on ethical AI implementation in housing, promoting fairer and more accountable decision-making.    
    </p>
  </div>